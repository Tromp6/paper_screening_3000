# Systematic Literature Review Automation Platform

A comprehensive, AI-powered workflow for conducting **PRISMA-compliant systematic literature reviews** with automated paper collection, intelligent screening, and seamless reference management.

![Workflow Status](https://img.shields.io/badge/workflow-production%20ready-brightgreen)
![Python](https://img.shields.io/badge/python-3.8%2B-blue)
![AI Powered](https://img.shields.io/badge/AI-GPT--4o%20screening-orange)
![PRISMA](https://img.shields.io/badge/PRISMA-compliant-success)

## ğŸš€ **Quick Start - 3 Simple Commands**

### **1. Setup** (one time only)
```bash
# Install dependencies
pip install openai pyzotero requests tqdm python-dotenv

# Configure API keys
cp .env.example .env
# Edit .env with your OpenAI and Zotero API keys
```

### **2. Run Complete Workflow**
```bash
# Start new systematic review (fetches papers + AI screening)
python systematic_review.py 

python systematic_review.py --max-results 50

# Manual review of AI decisions 
python systematic_review.py --manual-screen --run review_run_YYYYMMDD_HHMMSS

# Import approved papers to Zotero
python systematic_review.py --import-to-zotero --run review_run_YYYYMMDD_HHMMSS
```

### **3. View Results**
```bash
# Check status and results
python systematic_review.py --view-results --run review_run_YYYYMMDD_HHMMSS
```

**That's it!** ğŸ‰ You'll have a complete PRISMA-compliant systematic review with organized results.

---

## ğŸ¯ **What This Does**

This platform automates the time-intensive aspects of systematic literature reviews while maintaining rigorous academic standards. Perfect for researchers conducting reviews on **LLM security, jailbreak techniques, and defensive countermeasures**.

### **Key Features**

- ğŸ” **Automated Paper Collection** - Strategic search across OpenAlex (250M+ papers)
- ğŸ¤– **AI-Powered Screening** - GPT-4o evaluates paper relevance with confidence scores
- ğŸ‘¤ **Human Oversight Interface** - Manual review of AI decisions with easy Accept/Reject workflow
- ğŸ“Š **PRISMA Documentation** - Automatic generation of compliant flow diagrams and reports
- ğŸ“š **Zotero Integration** - Direct import of approved papers with metadata
- ğŸ”„ **Full Reproducibility** - Complete audit trail and search documentation

---

## ğŸ“‹ **Main Commands**

### **Start New Review**
```bash
# Basic usage - processes all 8 strategic queries
python systematic_review.py

# Limit papers per query (faster testing)
python systematic_review.py --max-results 10

# Start from specific query (resume capability)
python systematic_review.py --start-from 3
```

### **Manual Screening Options**
```bash
# Review AI decisions for existing run
python systematic_review.py --manual-screen --run review_run_20250605_140652

# When prompted, choose:
# 1 = Review only AI-included papers (recommended - efficient)
# 2 = Review only AI-rejected papers (check false negatives)  
# 3 = Review both included and rejected papers (comprehensive)
# 5 = Accept all AI decisions automatically
```

### **Import and Results**
```bash
# Import final approved papers to Zotero
python systematic_review.py --import-to-zotero --run review_run_20250605_140652

# View detailed statistics
python systematic_review.py --view-results --run review_run_20250605_140652

# Generate combined PRISMA report
python systematic_review.py --generate-report --run review_run_20250605_140652
```

### **ğŸ”„ Dynamic Query Management**

**One of the key advantages of this platform** - you can easily add or remove queries from an existing review and regenerate accurate reports!

```bash
# Add a new query to existing review
# 1. Update queries_config.json with new query (e.g., query #9)
# 2. Run only the new query
python systematic_review.py --start-from 9 --run review_run_20250605_140652

# 3. Regenerate complete report with all queries (including new one)
python systematic_review.py --generate-report --run review_run_20250605_140652

# Remove a query from existing review  
# 1. Delete the query folder
rm -rf systematic_review_results/review_run_20250605_140652/01_individual_queries/screening/query_03/

# 2. Regenerate report without that query
python systematic_review.py --generate-report --run review_run_20250605_140652
```

**Why This Works:**
- âœ… **Late-stage deduplication** - conflicts resolved at report generation
- âœ… **Dynamic query detection** - system automatically finds all existing queries  
- âœ… **Fresh calculations** - statistics and counts updated correctly
- âœ… **Conflict resolution** - handles duplicates between old and new queries
- âœ… **No re-screening required** - preserves all existing work

**Perfect for iterative research** where you discover new search terms or need to refine your scope!

---

## ğŸ“ **Output Structure**

After running, you'll get organized results:

```
systematic_review_results/
â””â”€â”€ review_run_20250605_140652/           # Your review run
    â”œâ”€â”€ README.md                         # Run documentation
    â”œâ”€â”€ 01_individual_queries/            # Query-by-query results
    â”‚   â”œâ”€â”€ papers/query_01/              # Papers from each query
    â”‚   â””â”€â”€ screening/query_01/           # AI screening results
    â”œâ”€â”€ 02_reports_generated/             # PRISMA flow diagrams
    â”œâ”€â”€ 03_zotero_imports/               # Final import files
    â””â”€â”€ 04_workflow_logs/                # Complete audit trail
```

---

## ğŸ”§ **Configuration**

### **API Keys Setup**

Create `.env` file:
```bash
# OpenAI API (required for AI screening)
OPENAI_API_KEY=your_openai_api_key

# Zotero API (optional - for automatic import)
ZOT_USER_ID=your_zotero_user_id
ZOT_KEY=your_zotero_api_key
COLL_OA=your_zotero_collection_id
```

### **Customizing Search Queries**

**ğŸ”§ All search queries are defined in `queries_config.json`** - you must customize this file for your research topic!

The default configuration is optimized for **LLM security research**, but you can easily adapt it for any systematic review topic.

#### **Query Structure**
```json
{
  "strategic_queries": [
    {
      "id": 1,
      "query": "\"jailbreak\" AND \"large language model\"",
      "description": "Direct jailbreak techniques for LLMs", 
      "category": "attack",
      "expected_focus": "Core jailbreak methodologies and prompt-based exploits"
    },
    {
      "id": 2,
      "query": "\"prompt injection\" AND \"machine learning\"",
      "description": "Prompt injection vulnerabilities",
      "category": "attack", 
      "expected_focus": "Input validation bypass techniques"
    }
  ]
}
```

#### **Key Configuration Options**

**Time Range:**
```json
"timeframe": {
  "start_date": "2022-01-01",    // Adjust for your field's relevant period
  "end_date": "2025-01-01"       // Current date boundary
}
```

**Quality Filters:**
```json
"quality_criteria": {
  "min_citations": 5,            // Higher for mature fields, lower for emerging topics
  "peer_reviewed_only": true,    // Set false to include preprints
  "exclude_preprints": true      // Set false for cutting-edge research
}
```

#### **Example: Medical Research Configuration**
```json
{
  "search_strategy": {
    "version": "v1.0_medical_intervention",
    "description": "Systematic review of telemedicine interventions",
    "timeframe": {
      "start_date": "2020-01-01",
      "end_date": "2025-01-01"
    }
  },
  "strategic_queries": [
    {
      "id": 1,
      "query": "\"telemedicine\" AND \"patient outcomes\"",
      "description": "Telemedicine effectiveness studies",
      "category": "intervention",
      "expected_focus": "Clinical outcomes and patient satisfaction"
    },
    {
      "id": 2, 
      "query": "\"remote consultation\" AND \"healthcare quality\"",
      "description": "Quality of remote healthcare delivery",
      "category": "quality",
      "expected_focus": "Healthcare delivery standards and metrics"
    }
  ]
}
```

#### **Tips for Effective Query Design**

**âœ… Best Practices:**
- Use 3-8 strategic queries (not too few, not overwhelming)
- Combine specific terms with broader concepts: `"blockchain" AND "supply chain"`
- Use quotes for exact phrases: `"machine learning"`
- Balance breadth vs precision based on your field's maturity

**âŒ Common Pitfalls:**
- Don't use overly complex Boolean logic (AI screening compensates for broad queries)
- Avoid queries that are too narrow (might miss relevant papers)
- Don't duplicate concepts across queries (creates unnecessary overlap)

**ğŸ¯ The platform handles deduplication automatically**, so slightly overlapping queries are fine!

### **Search Configuration**

The system uses 8 strategic queries optimized for LLM security research (defined in `queries_config.json`):

**Attack Techniques (3 queries):**
- `"jailbreak" AND "large language model"`
- `"prompt injection" AND "large language model"`  
- `"adversarial prompt" AND "large language model"`

**Defense Strategies (5 queries):**
- `"defend" AND "jailbreak"`
- `"prompt injection defense"`
- `"defense mechanism" AND "language model"`
- `"jailbreak detection"`
- `"security framework" AND "language model"`

**Search Period:** 2022-2025 (modern LLM era)  
**Quality Filter:** 5+ citations for impact assessment

---

## ğŸ“Š **Example Output**

### **Successful Run Results**
```
ğŸ“Š SYSTEMATIC REVIEW RESULTS SUMMARY
================================================================================
âœ… Processing Query #1: 70 total results found â†’ 5 papers included
âœ… Processing Query #2: 45 total results found â†’ 3 papers included  
âœ… Processing Query #3: 25 total results found â†’ 2 papers included

ğŸ“Š OVERALL TOTALS:
   ğŸ“„ Papers collected: 140
   ğŸ¤– AI included: 68
   âŒ AI excluded: 72
   ğŸ‘¤ Manually reviewed: 68
   ğŸ“ˆ Final approved: 65
   ğŸ“š Imported to Zotero: 65 (100% success rate)
```

### **Manual Screening Interface**
```
[1/35] Paper:
ğŸ“„ Title: Multi-step Jailbreaking Privacy Attacks on ChatGPT
ğŸ‘¥ Authors: Haoran Li, Dadi Guo, Wei Fan
ğŸ“ Venue: EMNLP Findings (2023)
ğŸŸ¢ AI Decision: INCLUDE (confidence: 0.95)
ğŸ’­ AI Reason: Explicitly mentions jailbreaking attacks on ChatGPT

ğŸ‘¤ Your decision [A]ccept/[R]eject/[S]kip: A
âœ… Confirmed for inclusion
```

---

## ğŸ—‚ï¸ **Project Files**

```
vacuum/
â”œâ”€â”€ README.md                    # This guide
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ .env.example                 # Environment template
â”œâ”€â”€ queries_config.json          # Search strategy configuration
â”‚
â”œâ”€â”€ systematic_review.py         # ğŸ¯ MAIN ENTRY POINT - run this!
â”œâ”€â”€ ai_screening.py              # ğŸ¤– AI-powered paper screening
â”œâ”€â”€ screening_workflow.py        # ğŸ“Š Paper collection utilities  
â””â”€â”€ prisma_generator.py          # ğŸ“‹ PRISMA documentation generator
```

**Main entry point:** `systematic_review.py` - this orchestrates everything!

## ğŸ› **Troubleshooting**

### **Common Issues**

**"No papers found"** - Check API connectivity:
```bash
# Test basic functionality
python systematic_review.py --max-results 2
```

**Manual screening interrupted** - Resume where you left off:
```bash
# Progress is automatically saved
python systematic_review.py --manual-screen --run review_run_20250605_140652
```

**Missing API keys**:
```bash
# Verify configuration
python -c "import openai; print('âœ… OpenAI configured')"
```

**Dependencies missing**:
```bash
pip install openai pyzotero requests tqdm python-dotenv
```

---

## ğŸ“ˆ **Performance**

**Typical workflow for 300+ papers:**
- **Paper Collection**: ~2-5 minutes  
- **AI Screening**: ~10-15 minutes
- **Manual Screening**: ~30-60 minutes (68 pre-selected papers)
- **Zotero Import**: ~2-3 minutes
- **Total Time**: ~45-75 minutes for complete review

**Quality metrics:**
- **AI Accuracy**: ~90-95% agreement with human reviewers
- **PRISMA Compliance**: 100% with automated documentation
- **Reproducibility**: Complete with versioned searches

---


**ğŸ¯ Built for researchers, by researchers - making systematic literature reviews efficient, reproducible, and rigorous.** 